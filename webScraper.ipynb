{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport requests \\n# Download wikipage \\nwikipage = 'https://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_by_continent_(data_file)'\\nresult = requests.get(wikipage)\\nresult.content\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import requests \n",
    "# Download wikipage \n",
    "wikipage = 'https://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_by_continent_(data_file)'\n",
    "result = requests.get(wikipage)\n",
    "result.content\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\nimport pandas as pd\\nfrom bs4 import BeautifulSoup\\n\\n# download wikipage\\nwikipage = \"https://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_by_continent_(data_file)\"\\nresult = requests.get(wikipage)\\n\\n# if successful parse the download into a BeautifulSoup object, which allows easy manipulation \\nif result.status_code == 200:\\n    soup = BeautifulSoup(result.content, \"html.parser\")\\n    \\n# find the object with HTML class wikitable sortable\\ntable = soup.find(\\'table\\',{\\'class\\':\\'wikitable sortable\\'})\\n\\n# loop through all the rows and pull the text\\nnew_table = []\\nfor row in table.find_all(\\'tr\\')[1:]:\\n    column_marker = 0\\n    columns = row.find_all(\\'td\\')\\n    new_table.append([column.get_text() for column in columns])\\n    \\ndf = pd.DataFrame(new_table, columns=[\\'ContinentCode\\',\\'Alpha2\\',\\'Alpha3\\',\\'PhoneCode\\',\\'Name\\'])\\ndf[\\'Name\\'] = df[\\'Name\\'].str.replace(\\'\\n\\',\\'\\')\\ndf\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# download wikipage\n",
    "wikipage = \"https://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_by_continent_(data_file)\"\n",
    "result = requests.get(wikipage)\n",
    "\n",
    "# if successful parse the download into a BeautifulSoup object, which allows easy manipulation \n",
    "if result.status_code == 200:\n",
    "    soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "    \n",
    "# find the object with HTML class wikitable sortable\n",
    "table = soup.find('table',{'class':'wikitable sortable'})\n",
    "\n",
    "# loop through all the rows and pull the text\n",
    "new_table = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    column_marker = 0\n",
    "    columns = row.find_all('td')\n",
    "    new_table.append([column.get_text() for column in columns])\n",
    "    \n",
    "df = pd.DataFrame(new_table, columns=['ContinentCode','Alpha2','Alpha3','PhoneCode','Name'])\n",
    "df['Name'] = df['Name'].str.replace('\\n','')\n",
    "df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib import parse\n",
    "from urllib import request as req\n",
    "import requests\n",
    "import bs4\n",
    "import hashlib\n",
    "import selenium\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH='/Users/logan/Documents/Data Mining/Project/chromedriver'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwd = webdriver.Chrome(executable_path=DRIVER_PATH)\\nwd.get('https://google.com')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "wd = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "wd.get('https://google.com')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsearch_box = wd.find_element_by_css_selector('input.gLFyf')\\nsearch_box.send_keys('Dogs')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
    "search_box.send_keys('Dogs')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\AppData\\Local\\Temp/ipykernel_2320/539653837.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  wd = webdriver.Chrome(executable_path=DRIVER_PATH)\n"
     ]
    }
   ],
   "source": [
    "wd = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_image_urls(query:str, max_links_to_fetch:int, wd:webdriver, sleep_between_interactions:int=1):\n",
    "    def scroll_to_end(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_between_interactions)    \n",
    "    \n",
    "    # build the google query\n",
    "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "    # load the page\n",
    "    wd.get(search_url.format(q=query))\n",
    "\n",
    "    image_urls = set()\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "    while image_count < max_links_to_fetch:\n",
    "        scroll_to_end(wd)\n",
    "\n",
    "        # get all image thumbnail results\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "        number_results = len(thumbnail_results)\n",
    "        \n",
    "        print(f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\")\n",
    "        \n",
    "        for img in thumbnail_results[results_start:number_results]:\n",
    "            # try to click every thumbnail such that we can get the real image behind it\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(sleep_between_interactions)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # extract image urls    \n",
    "            actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                    image_urls.add(actual_image.get_attribute('src'))\n",
    "\n",
    "            image_count = len(image_urls)\n",
    "\n",
    "            if len(image_urls) >= max_links_to_fetch:\n",
    "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "            time.sleep(30)\n",
    "            return\n",
    "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "            if load_more_button:\n",
    "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "\n",
    "        # move the result startpoint further down\n",
    "        results_start = len(thumbnail_results)\n",
    "\n",
    "    return image_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def persist_image(folder_path:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_download(search_term:str,driver_path:str,target_path='./images',number_images=100):\n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    with webdriver.Chrome(executable_path=driver_path) as wd:\n",
    "        res = fetch_image_urls(search_term, number_images, wd=wd, sleep_between_interactions=0.5)\n",
    "        \n",
    "    for elem in res:\n",
    "        persist_image(target_folder,elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Carpenter bees\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\AppData\\Local\\Temp/ipykernel_2320/640344845.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  with webdriver.Chrome(executable_path=driver_path) as wd:\n",
      "C:\\Users\\logan\\AppData\\Local\\Temp/ipykernel_2320/322778573.py:19: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 100 search results. Extracting links from 0:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\AppData\\Local\\Temp/ipykernel_2320/322778573.py:33: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 32 image links, looking for more ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2320/2851603147.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m search_and_download(search_term=search_term,\n\u001b[0m\u001b[0;32m      2\u001b[0m                    driver_path = DRIVER_PATH)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2320/640344845.py\u001b[0m in \u001b[0;36msearch_and_download\u001b[1;34m(search_term, driver_path, target_path, number_images)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_image_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msleep_between_interactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mpersist_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "search_and_download(search_term=search_term,\n",
    "                   driver_path = DRIVER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
